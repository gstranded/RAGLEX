# -*- coding: utf-8 -*-
"""
å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•å‡½æ•°
æµ‹è¯•åŒ…æ‹¬ï¼šé—®é¢˜è¡¥å…¨ã€ä¸Šä¸‹æ–‡è¡¥å…¨ã€æ„å›¾è¯†åˆ«ã€å¤šæŸ¥è¯¢ç”Ÿæˆã€BM25+å‘é‡æ£€ç´¢ã€é‡æ’åºã€è”ç½‘æ£€ç´¢ã€æœ€ç»ˆå›ç­”ç”Ÿæˆ
"""

import os
import sys
import numpy as np
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv
from rank_bm25 import BM25Okapi

# å¯¼å…¥ç³»ç»Ÿç»„ä»¶
from utils import (
    get_vectorstore, get_model_openai, get_memory, 
    rerank_documents_doc, get_embeder,
    get_law_vectorstore, get_case_vectorstore,
    search_law_documents, search_case_documents,
    index_all_documents_separated
)
from retriever import get_multi_query_law_retiever
from prompt import (
    PRE_QUESTION_PROMPT, CHECK_INTENT_PROMPT, 
    LAW_PROMPT_HISTORY, FRIENDLY_REJECTION_PROMPT,
    MULTI_QUERY_PROMPT_TEMPLATE
)
from combine import combine_law_docs
# ç½‘ç»œæœç´¢åŠŸèƒ½å®ç°
def search_web_serper(query: str, num_results: int = 3) -> str:
    """
    ä½¿ç”¨Serper APIè¿›è¡Œç½‘ç»œæœç´¢
    
    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        num_results: è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º3
        
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœå­—ç¬¦ä¸²
    """
    import requests
    import json
    import os
    
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–APIå¯†é’¥
        serper_api_key = os.getenv('SERPER_API_KEY')
        if not serper_api_key:
            print("è­¦å‘Š: æœªæ‰¾åˆ°SERPER_API_KEYç¯å¢ƒå˜é‡")
            return "ç½‘ç»œæœç´¢ä¸å¯ç”¨ï¼šç¼ºå°‘APIå¯†é’¥"
        
        # Serper APIé…ç½®
        url = "https://google.serper.dev/search"
        headers = {
            'X-API-KEY': serper_api_key,
            'Content-Type': 'application/json'
        }
        
        # è¯·æ±‚æ•°æ®
        payload = {
            "q": query,
            "num": num_results,
            "hl": "zh-cn",  # ä¸­æ–‡æœç´¢
            "gl": "cn"      # ä¸­å›½åœ°åŒº
        }
        
        # å‘é€è¯·æ±‚
        response = requests.post(url, headers=headers, json=payload, timeout=10)
        response.raise_for_status()
        
        data = response.json()
        
        # è§£ææœç´¢ç»“æœ
        results = []
        if 'organic' in data:
            for item in data['organic'][:num_results]:
                title = item.get('title', 'æ— æ ‡é¢˜')
                snippet = item.get('snippet', 'æ— æ‘˜è¦')
                link = item.get('link', 'æ— é“¾æ¥')
                
                # æ ¼å¼åŒ–å•ä¸ªç»“æœ
                formatted_result = f"æ ‡é¢˜: {title}\næ‘˜è¦: {snippet}\næ¥æº: {link}"
                results.append(formatted_result)
        
        if results:
            # ç”¨åŒæ¢è¡Œç¬¦è¿æ¥å¤šä¸ªç»“æœ
            return "\n\n".join(results)
        else:
            return "æœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœ"
            
    except requests.exceptions.RequestException as e:
        print(f"ç½‘ç»œè¯·æ±‚é”™è¯¯: {e}")
        return "ç½‘ç»œæœç´¢å¤±è´¥ï¼šè¯·æ±‚é”™è¯¯"
    except json.JSONDecodeError as e:
        print(f"JSONè§£æé”™è¯¯: {e}")
        return "ç½‘ç»œæœç´¢å¤±è´¥ï¼šå“åº”è§£æé”™è¯¯"
    except Exception as e:
        print(f"æœç´¢è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return "ç½‘ç»œæœç´¢å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯"
from langchain_core.documents import Document
from langchain.schema.output_parser import StrOutputParser
from langchain.memory import ConversationBufferMemory
from config import config

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class CompleteQASystemTester:
    """
    å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•ç±»
    """
    
    def __init__(self, load_case_data=True):
        """åˆå§‹åŒ–æµ‹è¯•ç³»ç»Ÿ"""
        print("ğŸš€ åˆå§‹åŒ–å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•å™¨...")
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.vectorstore = get_vectorstore(config.LAW_VS_COLLECTION_NAME)
        self.law_vectorstore = get_law_vectorstore()
        self.case_vectorstore = get_case_vectorstore()
        self.model = get_model_openai()
        self.memory = get_memory()
        
        # å¦‚æœéœ€è¦åŠ è½½æ¡ˆä¾‹æ•°æ®
        if load_case_data:
            self._load_case_data()
        
        # åˆå§‹åŒ–BM25ç´¢å¼•
        self.bm25_index = self._create_bm25_index()
        
        # åˆå§‹åŒ–å¤šæŸ¥è¯¢æ£€ç´¢å™¨
        vs_retriever = self.vectorstore.as_retriever(search_kwargs={"k": config.LAW_VS_SEARCH_K})
        self.multi_query_retriever = get_multi_query_law_retiever(vs_retriever, self.model)
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _load_case_data(self):
        """åŠ è½½æ¡ˆä¾‹æ•°æ®åˆ°æ•°æ®åº“"""
        print("ğŸ“š æ£€æŸ¥å¹¶åŠ è½½æ¡ˆä¾‹æ•°æ®...")
        try:
            # æ£€æŸ¥æ¡ˆä¾‹æ•°æ®åº“æ˜¯å¦å·²æœ‰æ•°æ®
            case_docs = self.case_vectorstore.similarity_search("", k=1)
            if case_docs:
                print(f"âœ… æ¡ˆä¾‹æ•°æ®åº“å·²å­˜åœ¨æ•°æ®ï¼ŒåŒ…å«æ–‡æ¡£æ•°é‡: {len(case_docs)}")
                return
            
            print("ğŸ“¥ æ¡ˆä¾‹æ•°æ®åº“ä¸ºç©ºï¼Œå¼€å§‹åŠ è½½æ¡ˆä¾‹æ•°æ®...")
            
            # åŠ è½½æ–‡æ¡£
            from loader import load_law_documents_only, load_case_documents_only
            
            print("ğŸ“š åŠ è½½æ³•å¾‹æ¡æ–‡æ–‡æ¡£...")
            law_docs = load_law_documents_only()
            print(f"âœ… åŠ è½½äº† {len(law_docs)} ä¸ªæ³•å¾‹æ¡æ–‡æ–‡æ¡£")
            
            print("âš–ï¸  åŠ è½½æ¡ˆä¾‹æ–‡æ¡£...")
            case_docs = load_case_documents_only()
            print(f"âœ… åŠ è½½äº† {len(case_docs)} ä¸ªæ¡ˆä¾‹æ–‡æ¡£")
            
            # ä½¿ç”¨åˆ†ç¦»ç´¢å¼•åŠŸèƒ½åŠ è½½æ•°æ®
            index_all_documents_separated(law_docs, case_docs)
            print("âœ… æ¡ˆä¾‹æ•°æ®åŠ è½½å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ æ¡ˆä¾‹æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _create_bm25_index(self) -> BM25Okapi:
        """åˆ›å»ºBM25ç´¢å¼•"""
        print("ğŸ“š åˆ›å»ºBM25ç´¢å¼•...")
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_docs = self.vectorstore.similarity_search("", k=1000)  # è·å–å¤§é‡æ–‡æ¡£ç”¨äºBM25
            
            # é¢„å¤„ç†æ–‡æ¡£æ–‡æœ¬
            tokenized_docs = []
            for doc in all_docs:
                # æ¸…ç†æ–‡æœ¬ï¼Œä¿ç•™ä¸­æ–‡åˆ†è¯
                text = doc.page_content.replace('\t', ' ').replace('\n', ' ')
                # å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œå­—ç¬¦çº§åˆ†è¯
                tokens = list(text.replace(' ', ''))
                # è¿‡æ»¤ç©ºå­—ç¬¦
                tokens = [token for token in tokens if token.strip()]
                if tokens:  # ç¡®ä¿ä¸ä¸ºç©º
                    tokenized_docs.append(tokens)
            
            bm25 = BM25Okapi(tokenized_docs)
            print(f"âœ… BM25ç´¢å¼•åˆ›å»ºå®Œæˆï¼ŒåŒ…å« {len(tokenized_docs)} ä¸ªæ–‡æ¡£")
            return bm25
        except Exception as e:
            print(f"âŒ BM25ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    def step1_question_completion(self, question: str, chat_history: str = "") -> str:
        """æ­¥éª¤1: é—®é¢˜è¡¥å…¨"""
        print("\nğŸ” æ­¥éª¤1: é—®é¢˜è¡¥å…¨")
        print(f"åŸå§‹é—®é¢˜: {question}")
        
        try:
            # æ„å»ºè¾“å…¥
            input_data = {
                "question": question,
                "chat_history": chat_history
            }
            
            # æ‰§è¡Œé—®é¢˜è¡¥å…¨
            chain = PRE_QUESTION_PROMPT | self.model | StrOutputParser()
            completed_question = chain.invoke(input_data)
            
            print(f"è¡¥å…¨åé—®é¢˜: {completed_question}")
            return completed_question
        except Exception as e:
            print(f"âŒ é—®é¢˜è¡¥å…¨å¤±è´¥: {e}")
            return question
    
    def step2_intent_recognition(self, question: str) -> str:
        """æ­¥éª¤2: æ„å›¾è¯†åˆ«"""
        print("\nğŸ¯ æ­¥éª¤2: æ„å›¾è¯†åˆ«")
        
        try:
            # æ‰§è¡Œæ„å›¾è¯†åˆ«
            chain = CHECK_INTENT_PROMPT | self.model | StrOutputParser()
            intent = chain.invoke({"question": question}).strip().lower()
            
            print(f"è¯†åˆ«æ„å›¾: {intent}")
            return intent
        except Exception as e:
            print(f"âŒ æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return "other"
    
    def step3_multi_query_generation(self, question: str) -> List[str]:
        """æ­¥éª¤3: ç”Ÿæˆå¤šæŸ¥è¯¢"""
        print("\nğŸ”„ æ­¥éª¤3: ç”Ÿæˆå¤šæŸ¥è¯¢")
        
        try:
            # æ‰§è¡Œå¤šæŸ¥è¯¢ç”Ÿæˆ
            chain = MULTI_QUERY_PROMPT_TEMPLATE | self.model | StrOutputParser()
            multi_queries_text = chain.invoke({"question": question})
            
            # è§£æå¤šä¸ªæŸ¥è¯¢
            queries = [line.strip() for line in multi_queries_text.strip().split("\n") if line.strip()]
            
            print("ç”Ÿæˆçš„å¤šæŸ¥è¯¢:")
            for i, query in enumerate(queries, 1):
                print(f"  {i}. {query}")
            
            return queries
        except Exception as e:
            print(f"âŒ å¤šæŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            return [question]
    
    def step4_separated_retrieval(self, question: str, law_k: int = 10, case_k: int = 5) -> List[Document]:
        """æ­¥éª¤4: åˆ†ç¦»æ£€ç´¢ - å…ˆæ£€ç´¢æ³•å¾‹æ¡æ–‡ï¼Œå†æ£€ç´¢æ¡ˆä¾‹"""
        print("\nğŸ“– æ­¥éª¤4: åˆ†ç¦»æ£€ç´¢ (æ³•å¾‹æ¡æ–‡ + æ¡ˆä¾‹)")
        print(f"æŸ¥è¯¢é—®é¢˜: {question}")
        print(f"æ³•å¾‹æ¡æ–‡æ£€ç´¢æ•°é‡: {law_k}")
        print(f"æ¡ˆä¾‹æ£€ç´¢æ•°é‡: {case_k}")
        
        try:
            # 4.1 æ³•å¾‹æ¡æ–‡æ£€ç´¢é˜¶æ®µ
            print("\n  4.1 æ³•å¾‹æ¡æ–‡æ£€ç´¢é˜¶æ®µ")
            print("  " + "-" * 30)
            
            law_docs = search_law_documents(
                question=question,
                k=law_k,
                use_rerank=True
            )
            
            print(f"  æ³•å¾‹æ¡æ–‡æ£€ç´¢è·å¾— {len(law_docs)} ä¸ªæ–‡æ¡£")
            
            # æ˜¾ç¤ºæ³•å¾‹æ¡æ–‡æ£€ç´¢å‰3ä¸ªç»“æœ
            print("  æ³•å¾‹æ¡æ–‡æ£€ç´¢å‰3ä¸ªç»“æœ:")
            for i, doc in enumerate(law_docs[:3]):
                preview = doc.page_content[:80] + "..." if len(doc.page_content) > 80 else doc.page_content
                doc_type = doc.metadata.get('doc_type', 'æœªçŸ¥')
                print(f"    {i+1}. [{doc_type}] {preview}")
            
            # 4.2 æ¡ˆä¾‹æ£€ç´¢é˜¶æ®µ
            print("\n  4.2 æ¡ˆä¾‹æ£€ç´¢é˜¶æ®µ")
            print("  " + "-" * 30)
            
            case_docs = search_case_documents(
                question=question,
                k=case_k,
                use_rerank=True
            )
            
            print(f"  æ¡ˆä¾‹æ£€ç´¢è·å¾— {len(case_docs)} ä¸ªæ–‡æ¡£")
            
            # æ˜¾ç¤ºæ¡ˆä¾‹æ£€ç´¢å‰3ä¸ªç»“æœï¼ˆæ˜¾ç¤ºæå–çš„å…³é”®éƒ¨åˆ†ï¼‰
            print("  æ¡ˆä¾‹æ£€ç´¢å‰3ä¸ªç»“æœï¼ˆå·²æå–å…³é”®éƒ¨åˆ†ï¼‰:")
            for i, doc in enumerate(case_docs[:3]):
                # æ˜¾ç¤ºæ¡ˆä¾‹çš„å…³é”®éƒ¨åˆ†é¢„è§ˆ
                content_lines = doc.page_content.split('\n')
                preview_lines = []
                for line in content_lines[:5]:  # æ˜¾ç¤ºå‰5è¡Œ
                    if line.strip():
                        preview_lines.append(line.strip())
                preview = ' | '.join(preview_lines)
                if len(preview) > 100:
                    preview = preview[:100] + "..."
                
                doc_type = doc.metadata.get('doc_type', 'æœªçŸ¥')
                court = doc.metadata.get('court', 'æœªçŸ¥æ³•é™¢')
                print(f"    {i+1}. [{doc_type}] [{court}] {preview}")
                
                # æ˜¾ç¤ºåŒ…å«çš„å…³é”®éƒ¨åˆ†
                sections = []
                if '## åŸºæœ¬æ¡ˆæƒ…' in doc.page_content:
                    sections.append('åŸºæœ¬æ¡ˆæƒ…')
                if '## è£åˆ¤ç†ç”±' in doc.page_content:
                    sections.append('è£åˆ¤ç†ç”±')
                if '## è£åˆ¤è¦æ—¨' in doc.page_content:
                    sections.append('è£åˆ¤è¦æ—¨')
                if '## æ³•å¾‹æ¡æ–‡' in doc.page_content:
                    sections.append('æ³•å¾‹æ¡æ–‡')
                print(f"       åŒ…å«éƒ¨åˆ†: {', '.join(sections)}")
            
            # 4.3 åˆ†ç¦»å¤„ç†ç»“æœ
            print("\n  4.3 åˆ†ç¦»å¤„ç†ç»“æœ")
            print("  " + "-" * 30)
            
            # è¿”å›åˆ†ç¦»çš„ç»“æœå­—å…¸ï¼Œè€Œä¸æ˜¯åˆå¹¶çš„åˆ—è¡¨
            separated_results = {
                'law_docs': law_docs,
                'case_docs': case_docs,
                'total_count': len(law_docs) + len(case_docs)
            }
            
            print(f"  æ³•å¾‹æ¡æ–‡æ–‡æ¡£: {len(law_docs)} ä¸ª")
            print(f"  æ¡ˆä¾‹æ–‡æ¡£: {len(case_docs)} ä¸ªï¼ˆå·²æå–å…³é”®éƒ¨åˆ†ï¼‰")
            print(f"  æ€»æ–‡æ¡£æ•°é‡: {separated_results['total_count']} ä¸ª")
            
            return separated_results
                
        except Exception as e:
            print(f"âŒ åˆ†ç¦»æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _fusion_retrieval(self, query: str, k: int = 5, alpha: float = 0.5) -> List[Document]:
        """èåˆæ£€ç´¢å‡½æ•°ï¼ˆå‚è€ƒtestbm25.pyå®ç°ï¼‰"""
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£
            all_docs = self.vectorstore.similarity_search("", k=1000)
            chunk_count = len(all_docs)
            
            # è·å–BM25åˆ†æ•° - ä½¿ç”¨æ­£ç¡®çš„åˆ†è¯æ–¹å¼
            query_tokens = list(query.replace(' ', ''))
            query_tokens = [token for token in query_tokens if token.strip()]
            bm25_scores = self.bm25_index.get_scores(query_tokens)
            
            # ç¡®ä¿BM25åˆ†æ•°æ•°ç»„é•¿åº¦ä¸æ–‡æ¡£æ•°é‡ä¸€è‡´
            if len(bm25_scores) != chunk_count:
                print(f"    âš ï¸ BM25åˆ†æ•°æ•°é‡({len(bm25_scores)})ä¸æ–‡æ¡£æ•°é‡({chunk_count})ä¸åŒ¹é…")
                # è°ƒæ•´æ•°ç»„é•¿åº¦
                if len(bm25_scores) > chunk_count:
                    bm25_scores = bm25_scores[:chunk_count]
                else:
                    # è¡¥é½ä¸º0
                    bm25_scores = np.pad(bm25_scores, (0, chunk_count - len(bm25_scores)), 'constant')
            
            # ä½¿ç”¨å‘é‡æœç´¢è·å–ç›¸å…³æ–‡æ¡£åŠå…¶åˆ†æ•°
            vector_results = self.vectorstore.similarity_search_with_score(query, k=chunk_count)
            
            # å½’ä¸€åŒ–åˆ†æ•°
            vector_scores = np.array([score for _, score in vector_results])
            # å‘é‡åˆ†æ•°è½¬æ¢ï¼ˆè·ç¦»è¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼‰
            if np.max(vector_scores) > np.min(vector_scores):
                vector_scores = 1 - (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores))
            else:
                vector_scores = np.ones_like(vector_scores)
            
            # BM25åˆ†æ•°å½’ä¸€åŒ–
            if np.max(bm25_scores) > np.min(bm25_scores):
                bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores))
            else:
                bm25_scores = np.zeros_like(bm25_scores)
            
            print(f"    å‘é‡åˆ†æ•°å½’ä¸€åŒ–èŒƒå›´: {np.min(vector_scores):.4f} - {np.max(vector_scores):.4f}")
            print(f"    BM25åˆ†æ•°å½’ä¸€åŒ–èŒƒå›´: {np.min(bm25_scores):.4f} - {np.max(bm25_scores):.4f}")
            
            # ç»“åˆåˆ†æ•°
            combined_scores = alpha * vector_scores + (1 - alpha) * bm25_scores
            print(f"    èåˆæƒé‡: å‘é‡={alpha}, BM25={1-alpha}")
            print(f"    èåˆåˆ†æ•°èŒƒå›´: {np.min(combined_scores):.4f} - {np.max(combined_scores):.4f}")
            
            # æ’åºæ–‡æ¡£
            sorted_indices = np.argsort(combined_scores)[::-1]
            
            # è¿”å›å‰kä¸ªæ–‡æ¡£
            return [all_docs[i] for i in sorted_indices[:k]]
            
        except Exception as e:
            print(f"    âŒ èåˆæ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # é™çº§åˆ°å‘é‡æ£€ç´¢
            return self.vectorstore.similarity_search(query, k=k)
    
    def step5_reranking(self, question: str, docs: List[Document], top_k: int = 10) -> List[Document]:
        """æ­¥éª¤5: æ–‡æ¡£é‡æ’åº"""
        print("\nğŸ”„ æ­¥éª¤5: æ–‡æ¡£é‡æ’åº")
        print(f"è¾“å…¥æ–‡æ¡£æ•°é‡: {len(docs)}")
        print(f"ç›®æ ‡é‡æ’åºæ•°é‡: {top_k}")
        print(f"é‡æ’åºé—®é¢˜: {question}")
        
        try:
            print("\n  5.1 é‡æ’åºå‰æ–‡æ¡£é¢„è§ˆ")
            print("  " + "-" * 30)
            for i, doc in enumerate(docs[:5]):
                preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                print(f"    {i+1}. {preview}")
            
            print("\n  5.2 æ‰§è¡Œé‡æ’åº")
            print("  " + "-" * 30)
            from utils import rerank_existing_documents
            print("  æ­£åœ¨ä½¿ç”¨FlagRerankerè¿›è¡Œé‡æ’åº...")
            print(f"  è¾“å…¥æ–‡æ¡£æ•°é‡: {len(docs)}")
            print(f"  é‡æ’åºæ¨¡å‹è·¯å¾„: {config.RERANKER_PATH}")
            
            # æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨
            import os
            if not os.path.exists(config.RERANKER_PATH):
                print(f"  âŒ é‡æ’åºæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {config.RERANKER_PATH}")
                return docs[:top_k]
            
            reranked_docs = rerank_existing_documents(question, docs, top_k)
            print(f"  é‡æ’åºå‡½æ•°è¿”å›æ–‡æ¡£æ•°é‡: {len(reranked_docs)}")
            
            print(f"  é‡æ’åºå®Œæˆï¼Œè¿”å›å‰ {len(reranked_docs)} ä¸ªæ–‡æ¡£")
            
            print("\n  5.3 é‡æ’åºåæ–‡æ¡£é¢„è§ˆ")
            print("  " + "-" * 30)
            for i, doc in enumerate(reranked_docs[:5]):
                preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
                print(f"    {i+1}. {preview}")
            
            return reranked_docs
            
        except Exception as e:
            print(f"âŒ é‡æ’åºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            print(f"  å›é€€åˆ°åŸå§‹æ–‡æ¡£å‰{top_k}ä¸ª")
            return docs[:top_k]
    
    def step5_separated_reranking(self, question: str, separated_results: Dict, law_top_k: int = 6, case_top_k: int = 4) -> List[Document]:
        """æ­¥éª¤5: åˆ†ç¦»é‡æ’åº - åˆ†åˆ«å¯¹æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹è¿›è¡Œé‡æ’åº"""
        print("\nğŸ”„ æ­¥éª¤5: åˆ†ç¦»é‡æ’åº")
        print(f"é‡æ’åºé—®é¢˜: {question}")
        print(f"æ³•å¾‹æ¡æ–‡ç›®æ ‡æ•°é‡: {law_top_k}")
        print(f"æ¡ˆä¾‹ç›®æ ‡æ•°é‡: {case_top_k}")
        
        try:
            law_docs = separated_results['law_docs']
            case_docs = separated_results['case_docs']
            
            print(f"\nè¾“å…¥æ–‡æ¡£ç»Ÿè®¡:")
            print(f"  æ³•å¾‹æ¡æ–‡: {len(law_docs)} ä¸ª")
            print(f"  æ¡ˆä¾‹æ–‡æ¡£: {len(case_docs)} ä¸ª")
            
            # 5.1 æ³•å¾‹æ¡æ–‡é‡æ’åº
            print("\n  5.1 æ³•å¾‹æ¡æ–‡é‡æ’åº")
            print("  " + "-" * 30)
            
            reranked_law_docs = []
            if law_docs:
                print(f"  æ­£åœ¨å¯¹ {len(law_docs)} ä¸ªæ³•å¾‹æ¡æ–‡è¿›è¡Œé‡æ’åº...")
                from utils import rerank_existing_documents
                reranked_law_docs = rerank_existing_documents(question, law_docs, law_top_k)
                print(f"  æ³•å¾‹æ¡æ–‡é‡æ’åºå®Œæˆï¼Œè·å¾— {len(reranked_law_docs)} ä¸ªæ–‡æ¡£")
                
                # æ˜¾ç¤ºæ³•å¾‹æ¡æ–‡é‡æ’åºç»“æœé¢„è§ˆ
                print("  æ³•å¾‹æ¡æ–‡é‡æ’åºå‰3ä¸ªç»“æœ:")
                for i, doc in enumerate(reranked_law_docs[:3]):
                    preview = doc.page_content[:80] + "..." if len(doc.page_content) > 80 else doc.page_content
                    doc_type = doc.metadata.get('doc_type', 'æœªçŸ¥')
                    print(f"    {i+1}. [{doc_type}] {preview}")
            else:
                print("  æ— æ³•å¾‹æ¡æ–‡éœ€è¦é‡æ’åº")
            
            # 5.2 æ¡ˆä¾‹é‡æ’åº
            print("\n  5.2 æ¡ˆä¾‹é‡æ’åº")
            print("  " + "-" * 30)
            
            reranked_case_docs = []
            if case_docs:
                print(f"  æ­£åœ¨å¯¹ {len(case_docs)} ä¸ªæ¡ˆä¾‹è¿›è¡Œé‡æ’åº...")
                from utils import rerank_existing_documents
                reranked_case_docs = rerank_existing_documents(question, case_docs, case_top_k)
                print(f"  æ¡ˆä¾‹é‡æ’åºå®Œæˆï¼Œè·å¾— {len(reranked_case_docs)} ä¸ªæ–‡æ¡£")
                
                # æ˜¾ç¤ºæ¡ˆä¾‹é‡æ’åºç»“æœé¢„è§ˆ
                print("  æ¡ˆä¾‹é‡æ’åºå‰3ä¸ªç»“æœï¼ˆå·²æå–å…³é”®éƒ¨åˆ†ï¼‰:")
                for i, doc in enumerate(reranked_case_docs[:3]):
                    content_lines = doc.page_content.split('\n')
                    preview_lines = []
                    for line in content_lines[:3]:
                        if line.strip():
                            preview_lines.append(line.strip())
                    preview = ' | '.join(preview_lines)
                    if len(preview) > 100:
                        preview = preview[:100] + "..."
                    
                    doc_type = doc.metadata.get('doc_type', 'æœªçŸ¥')
                    court = doc.metadata.get('court', 'æœªçŸ¥æ³•é™¢')
                    print(f"    {i+1}. [{doc_type}] [{court}] {preview}")
                    
                    # æ˜¾ç¤ºåŒ…å«çš„å…³é”®éƒ¨åˆ†
                    sections = []
                    if '## åŸºæœ¬æ¡ˆæƒ…' in doc.page_content:
                        sections.append('åŸºæœ¬æ¡ˆæƒ…')
                    if '## è£åˆ¤ç†ç”±' in doc.page_content:
                        sections.append('è£åˆ¤ç†ç”±')
                    if '## è£åˆ¤è¦æ—¨' in doc.page_content:
                        sections.append('è£åˆ¤è¦æ—¨')
                    if '## æ³•å¾‹æ¡æ–‡' in doc.page_content:
                        sections.append('æ³•å¾‹æ¡æ–‡')
                    print(f"       åŒ…å«éƒ¨åˆ†: {', '.join(sections)}")
            else:
                print("  æ— æ¡ˆä¾‹éœ€è¦é‡æ’åº")
            
            # 5.3 åˆå¹¶é‡æ’åºç»“æœ
            print("\n  5.3 åˆå¹¶é‡æ’åºç»“æœ")
            print("  " + "-" * 30)
            
            # æ³•å¾‹æ¡æ–‡åœ¨å‰ï¼Œæ¡ˆä¾‹åœ¨å
            final_docs = reranked_law_docs + reranked_case_docs
            
            print(f"  æœ€ç»ˆæ–‡æ¡£æ•°é‡: {len(final_docs)}")
            print(f"  å…¶ä¸­æ³•å¾‹æ¡æ–‡: {len(reranked_law_docs)} ä¸ª")
            print(f"  å…¶ä¸­æ¡ˆä¾‹æ–‡æ¡£: {len(reranked_case_docs)} ä¸ªï¼ˆå·²æå–å…³é”®éƒ¨åˆ†ï¼‰")
            
            return final_docs
            
        except Exception as e:
            print(f"âŒ åˆ†ç¦»é‡æ’åºå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            # å›é€€åˆ°åŸå§‹æ–‡æ¡£
            law_docs = separated_results.get('law_docs', [])
            case_docs = separated_results.get('case_docs', [])
            return law_docs[:law_top_k] + case_docs[:case_top_k]
    
    def step6_web_search(self, question: str, num_results: int = 3) -> str:
        """æ­¥éª¤6: è”ç½‘æ£€ç´¢"""
        print("\nğŸŒ æ­¥éª¤6: è”ç½‘æ£€ç´¢")
        print(f"æœç´¢é—®é¢˜: {question}")
        print(f"ç›®æ ‡ç»“æœæ•°é‡: {num_results}")
        
        try:
            print("\n  6.1 æ‰§è¡Œç½‘ç»œæœç´¢")
            print("  " + "-" * 30)
            print("  æ­£åœ¨ä½¿ç”¨Serper APIè¿›è¡Œç½‘ç»œæœç´¢...")
            
            web_content = search_web_serper(question, num_results)
            
            if web_content:
                # æ£€æŸ¥web_contentæ˜¯å¦åŒ…å«å¤šä¸ªç»“æœï¼ˆé€šè¿‡æ¢è¡Œç¬¦åˆ†å‰²ï¼‰
                if "\n\n" in web_content:
                    # æŒ‰åŒæ¢è¡Œç¬¦åˆ†å‰²å¤šä¸ªç»“æœ
                    results_list = [r.strip() for r in web_content.split("\n\n") if r.strip()]
                else:
                    # å•ä¸ªç»“æœ
                    results_list = [web_content.strip()]
                
                print(f"  ç½‘ç»œæœç´¢å®Œæˆï¼Œè·å¾— {len(results_list)} ä¸ªç»“æœ")
                
                print("\n  6.2 ç½‘ç»œæœç´¢ç»“æœé¢„è§ˆ")
                print("  " + "-" * 30)
                for i, result in enumerate(results_list[:3], 1):
                    if result:
                        preview = result[:150] + "..." if len(result) > 150 else result
                        print(f"    ç»“æœ{i}: {preview}")
                        print("    " + "-" * 20)
                
                print(f"\n  ç½‘ç»œæœç´¢å†…å®¹æ€»é•¿åº¦: {len(web_content)} å­—ç¬¦")
                return web_content
            else:
                print("  âš ï¸ ç½‘ç»œæœç´¢æœªè¿”å›ç»“æœ")
                return ""
                
        except Exception as e:
            print(f"âŒ è”ç½‘æ£€ç´¢å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return "ç½‘ç»œæœç´¢ä¸å¯ç”¨"
    
    def step7_final_answer_generation(self, question: str, intent: str, context_docs: List[Document] = None, web_content: str = "") -> str:
        """æ­¥éª¤7: æœ€ç»ˆå›ç­”ç”Ÿæˆ"""
        print("\nğŸ’¡ æ­¥éª¤7: æœ€ç»ˆå›ç­”ç”Ÿæˆ")
        print(f"é—®é¢˜: {question}")
        print(f"æ„å›¾è¯†åˆ«ç»“æœ: {intent}")
        print(f"ä¸Šä¸‹æ–‡æ–‡æ¡£æ•°é‡: {len(context_docs) if context_docs else 0}")
        print(f"ç½‘ç»œå†…å®¹é•¿åº¦: {len(web_content)} å­—ç¬¦")
        
        try:
            if intent == "law":
                print("\n  7.1 æ³•å¾‹é—®ç­”å¤„ç†")
                print("  " + "-" * 30)
                
                # ä½¿ç”¨æ³•å¾‹é—®ç­”é“¾
                from chain import get_law_chain
                print("  æ­£åœ¨åˆå§‹åŒ–æ³•å¾‹é—®ç­”é“¾...")
                from callback import OutCallbackHandler
                out_callback = OutCallbackHandler()
                law_chain = get_law_chain(config, out_callback)
                
                # å‡†å¤‡ä¸Šä¸‹æ–‡
                print("  æ­£åœ¨å‡†å¤‡ä¸Šä¸‹æ–‡ä¿¡æ¯...")
                context = ""
                if context_docs:
                    context = "\n\n".join([doc.page_content for doc in context_docs])
                    print(f"  æœ¬åœ°çŸ¥è¯†åº“ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
                
                if web_content:
                    context += "\n\nç½‘ç»œæœç´¢ç»“æœ:\n" + web_content
                    print(f"  æ·»åŠ ç½‘ç»œæœç´¢å†…å®¹åæ€»é•¿åº¦: {len(context)} å­—ç¬¦")
                
                print("  æ­£åœ¨è°ƒç”¨æ³•å¾‹é—®ç­”æ¨¡å‹...")
                response = law_chain.invoke({
                    "question": question,
                    "context": context
                })
                
                # ä»å“åº”ä¸­æå–answerå­—æ®µ
                if isinstance(response, dict) and 'answer' in response:
                    final_answer = response['answer']
                else:
                    final_answer = str(response)
                
                print(f"  æ³•å¾‹é—®ç­”å®Œæˆï¼Œå›ç­”é•¿åº¦: {len(final_answer)} å­—ç¬¦")
                
                # æ˜¾ç¤ºå›ç­”é¢„è§ˆ
                print("\n  7.2 æ³•å¾‹å›ç­”é¢„è§ˆ")
                print("  " + "-" * 30)
                preview = final_answer[:200] + "..." if len(final_answer) > 200 else final_answer
                print(f"  {preview}")
                
                return final_answer
            else:
                print("\n  7.1 éæ³•å¾‹é—®é¢˜å¤„ç†")
                print("  " + "-" * 30)
                
                # éæ³•å¾‹é—®é¢˜çš„å‹å¥½æ‹’ç»
                from prompt import FRIENDLY_REJECTION_PROMPT
                from utils import get_model
                
                print("  æ­£åœ¨ç”Ÿæˆå‹å¥½æ‹’ç»å›ç­”...")
                model = get_model()
                response = model.invoke(FRIENDLY_REJECTION_PROMPT.format(question=question))
                
                final_response = response.content if hasattr(response, 'content') else str(response)
                print(f"  å‹å¥½æ‹’ç»å›ç­”å®Œæˆï¼Œé•¿åº¦: {len(final_response)} å­—ç¬¦")
                
                # æ˜¾ç¤ºå›ç­”é¢„è§ˆ
                print("\n  7.2 æ‹’ç»å›ç­”é¢„è§ˆ")
                print("  " + "-" * 30)
                preview = final_response[:200] + "..." if len(final_response) > 200 else final_response
                print(f"  {preview}")
                
                return final_response
                
        except Exception as e:
            print(f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚"
    
    def complete_qa_test(self, question: str, chat_history: str = "") -> Dict[str, Any]:
        """å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•"""
        print("="*80)
        print("ğŸ§ª å¼€å§‹å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•")
        print("="*80)
        
        results = {
            "original_question": question,
            "chat_history": chat_history
        }
        
        # æ­¥éª¤1: é—®é¢˜è¡¥å…¨
        completed_question = self.step1_question_completion(question, chat_history)
        results["completed_question"] = completed_question
        
        # æ­¥éª¤2: æ„å›¾è¯†åˆ«
        intent = self.step2_intent_recognition(completed_question)
        results["intent"] = intent
        
        # å¦‚æœæ˜¯æ³•å¾‹é—®é¢˜ï¼Œç»§ç»­å®Œæ•´æµç¨‹
        if intent == "law":
            # æ­¥éª¤3: å¤šæŸ¥è¯¢ç”Ÿæˆ
            multi_queries = self.step3_multi_query_generation(completed_question)
            results["multi_queries"] = multi_queries
            
            # æ­¥éª¤4: åˆ†ç¦»æ£€ç´¢ (æ³•å¾‹æ¡æ–‡ + æ¡ˆä¾‹)
            separated_results = self.step4_separated_retrieval(completed_question)
            results["retrieved_docs_count"] = separated_results['total_count']
            results["law_docs_count"] = len(separated_results['law_docs'])
            results["case_docs_count"] = len(separated_results['case_docs'])
            
            # æ­¥éª¤5: åˆ†ç¦»é‡æ’åº - åˆ†åˆ«å¯¹æ³•å¾‹æ¡æ–‡å’Œæ¡ˆä¾‹è¿›è¡Œé‡æ’åº
            reranked_docs = self.step5_separated_reranking(completed_question, separated_results)
            results["reranked_docs"] = reranked_docs
            results["reranked_docs_count"] = len(reranked_docs)
            
            # æ­¥éª¤6: è”ç½‘æ£€ç´¢
            web_content = self.step6_web_search(completed_question)
            results["web_content"] = web_content
            
            # æ­¥éª¤7: æœ€ç»ˆå›ç­”ç”Ÿæˆ
            final_answer = self.step7_final_answer_generation(
                completed_question, intent, reranked_docs, web_content
            )
            results["final_answer"] = final_answer
        else:
            # éæ³•å¾‹é—®é¢˜ç›´æ¥ç”Ÿæˆå‹å¥½å›åº”
            results["multi_queries"] = []
            results["retrieved_docs_count"] = 0
            results["reranked_docs"] = []
            results["web_content"] = ""
            
            final_answer = self.step7_final_answer_generation(
                completed_question, intent, [], ""
            )
            results["final_answer"] = final_answer
        
        # ä¿å­˜åˆ°è®°å¿†
        self.memory.save_context(
            {"question": completed_question}, 
            {"answer": results["final_answer"]}
        )
        
        print("\n" + "="*80)
        print("ğŸ‰ å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•å®Œæˆ")
        print("="*80)
        
        return results
    
    def print_test_summary(self, results: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦")
        print("=" * 80)
        
        # åŸºæœ¬ä¿¡æ¯
        print(f"ğŸ• æµ‹è¯•æ—¶é—´: {results.get('timestamp', 'N/A')}")
        print(f"â“ åŸå§‹é—®é¢˜: {results['original_question']}")
        print(f"ğŸ’¬ å¯¹è¯å†å²: {results.get('chat_history', 'æ— ') or 'æ— '}")
        
        print("\n" + "-" * 40)
        print("ğŸ” å¤„ç†æµç¨‹ç»Ÿè®¡")
        print("-" * 40)
        
        print(f"âœï¸  é—®é¢˜è¡¥å…¨: {results.get('completed_question', 'N/A')}")
        print(f"ğŸ¯ æ„å›¾è¯†åˆ«: {results.get('intent', 'N/A')}")
        
        multi_queries = results.get('multi_queries', [])
        print(f"ğŸ”„ å¤šæŸ¥è¯¢ç”Ÿæˆ: {len(multi_queries)} ä¸ªæŸ¥è¯¢")
        if multi_queries:
            for i, query in enumerate(multi_queries[:3], 1):
                print(f"    {i}. {query}")
            if len(multi_queries) > 3:
                print(f"    ... è¿˜æœ‰ {len(multi_queries) - 3} ä¸ªæŸ¥è¯¢")
        
        print(f"ğŸ“š åˆ†ç¦»æ£€ç´¢: {results.get('retrieved_docs_count', 0)} ä¸ªæ–‡æ¡£ (æ³•å¾‹æ¡æ–‡+æ¡ˆä¾‹)")
        print(f"ğŸ”„ é‡æ’åºå: {results.get('reranked_docs_count', 0)} ä¸ªæ–‡æ¡£")
        print(f"ğŸŒ ç½‘ç»œæœç´¢: {results.get('web_content_length', 0)} å­—ç¬¦")
        print(f"ğŸ’¡ æœ€ç»ˆå›ç­”: {results.get('final_answer_length', 0)} å­—ç¬¦")
        
        # é”™è¯¯ä¿¡æ¯
        if 'error' in results:
            print(f"\nâŒ é”™è¯¯ä¿¡æ¯: {results['error']}")
        
        print("\n" + "-" * 40)
        print("ğŸ“ æœ€ç»ˆå›ç­”")
        print("-" * 40)
        final_answer = results.get('final_answer', 'æ— å›ç­”')
        if len(final_answer) > 500:
            print(final_answer[:500] + "\n... (å›ç­”å·²æˆªæ–­ï¼Œå®Œæ•´å†…å®¹è¯·æŸ¥çœ‹è¿”å›ç»“æœ)")
        else:
            print(final_answer)
        
        print("\n" + "=" * 80)


def run_test_cases():
    """è¿è¡Œæµ‹è¯•ç”¨ä¾‹"""
    tester = CompleteQASystemTester()
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "question": "åˆåŒè¿çº¦æ€ä¹ˆå¤„ç†ï¼Ÿ",
            "chat_history": "",
            "description": "æ³•å¾‹é—®é¢˜æµ‹è¯• - åˆ†ç¦»æ£€ç´¢(æ³•å¾‹æ¡æ–‡+æ¡ˆä¾‹)"
        },
        {
            "question": "äº¤é€šäº‹æ•…è´£ä»»å¦‚ä½•è®¤å®šï¼Ÿ",
            "chat_history": "",
            "description": "æ³•å¾‹é—®é¢˜æµ‹è¯• - äº¤é€šäº‹æ•…ç›¸å…³æ¡ˆä¾‹æ£€ç´¢"
        },
        {
            "question": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "chat_history": "",
            "description": "éæ³•å¾‹é—®é¢˜æµ‹è¯•"
        },
        {
            "question": "è¿™ç§æƒ…å†µä¸‹çš„èµ”å¿æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "chat_history": "Human: æˆ‘çš„è½¦è¢«è¿½å°¾äº†\nAI: æ ¹æ®äº¤é€šäº‹æ•…å¤„ç†ç›¸å…³æ³•å¾‹...",
            "description": "ä¸Šä¸‹æ–‡ä¾èµ–é—®é¢˜æµ‹è¯• - åˆ†ç¦»æ£€ç´¢"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ§ª æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['description']}")
        
        results = tester.complete_qa_test(
            test_case["question"], 
            test_case["chat_history"]
        )
        
        tester.print_test_summary(results)       
        
        print("\n" + "="*100)


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨å®Œæ•´é—®ç­”ç³»ç»Ÿæµ‹è¯•")
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    required_env_vars = ["OPENAI_API_KEY", "SERPER_API_KEY"]
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    
    if missing_vars:
        print(f"âŒ ç¼ºå°‘ç¯å¢ƒå˜é‡: {', '.join(missing_vars)}")
        print("è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®è¿™äº›å˜é‡")
    else:
        print("âœ… ç¯å¢ƒå˜é‡æ£€æŸ¥é€šè¿‡")
        
        try:
            run_test_cases()
        except KeyboardInterrupt:
            print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()